{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/session-4/langchain-vs-langgraph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-langchain-vs-langgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain vs LangGraph: Web Search Q&A\n",
    "\n",
    "## üéØ The Challenge\n",
    "\n",
    "This notebook demonstrates why **LangGraph is essential for interactive Q&A workflows** that require follow-up questions, verification, and user interaction.\n",
    "\n",
    "### üß† Real-World Scenario\n",
    "**Build an Intelligent Q&A Assistant** that:\n",
    "\n",
    "1. üìù **Gets the user's question**\n",
    "2. üîç **Searches the web using Tavily**\n",
    "3. üìä **Analyzes and summarizes findings**\n",
    "4. üëÄ **Presents answer to user**\n",
    "5. üîÑ **Handles follow-up interactions:**\n",
    "   - User asks clarifying questions\n",
    "   - User requests more specific information\n",
    "   - User wants to verify sources\n",
    "   - User explores related topics\n",
    "\n",
    "This type of **conversational search** is everywhere:\n",
    "- ChatGPT with web search\n",
    "- Perplexity AI\n",
    "- Google's AI Overviews\n",
    "- Customer support chatbots\n",
    "\n",
    "Let's see how each approach handles this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python langchain-tavily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded:\n",
      "OPENAI_API_KEY: ‚úì Set\n",
      "TAVILY_API_KEY: ‚úì Set\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the keys are loaded (optional - remove in production)\n",
    "print(\"‚úÖ Environment variables loaded:\")\n",
    "print(f\"OPENAI_API_KEY: {'‚úì Set' if os.environ.get('OPENAI_API_KEY') else '‚úó Missing'}\")\n",
    "print(f\"TAVILY_API_KEY: {'‚úì Set' if os.environ.get('TAVILY_API_KEY') else '‚úó Missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/q4bp4bs504v2x945rq65zhsw0000gn/T/ipykernel_64666/357933019.py:11: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  search_tool = TavilySearchResults(max_results=5)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing import Dict, List, Annotated\n",
    "import operator\n",
    "\n",
    "# Initialize LLM and search tool\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "search_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Example: Interactive Web Search Q&A\n",
    "\n",
    "## üéØ Task Flow:\n",
    "```\n",
    "Question ‚Üí Search ‚Üí Analyze ‚Üí Answer ‚Üí [Follow-up | New Topic]\n",
    "             ‚Üë                           ‚Üì\n",
    "             ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê ‚Üê\n",
    "```\n",
    "\n",
    "**Key Challenge**: How do you handle **conversational context**, **follow-up questions**, and **multi-turn interactions**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è A. LangChain Approach (Linear Chains)\n",
    "\n",
    "## üò´ The Problem: No Memory or Context Between Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/q4bp4bs504v2x945rq65zhsw0000gn/T/ipykernel_64666/2786253218.py:20: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  self.analyze_chain = LLMChain(llm=llm, prompt=self.analyze_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîó LANGCHAIN: Linear Q&A (No Context)\n",
      "============================================================\n",
      "üîç Searching for: What are the latest developments in AI in 2024?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/q4bp4bs504v2x945rq65zhsw0000gn/T/ipykernel_64666/2786253218.py:38: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = self.analyze_chain.run(question=question, search_results=formatted_results)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: In 2024, the field of artificial intelligence (AI) has witnessed significant advancements across various domains, reflecting a blend of innovation, practical applications, and a focus on responsible development. Here are the key developments:\n",
      "\n",
      "1. **Google's Gemini 2.0**: Google made a major leap wit...\n",
      "\n",
      "============================================================\n",
      "üîó LANGCHAIN: Conversation Attempt (Context Lost)\n",
      "============================================================\n",
      "ü§ñ LangChain: Trying conversation (no memory!)...\n",
      "\n",
      "--- Question 1 ---\n",
      "üîç Searching for: What are the latest developments in AI in 2024?\n",
      "Answer: In 2024, the field of artificial intelligence (AI) has seen remarkable advancements across various domains, driven by a focus on generative AI, smaller and more efficient models, and the integration o...\n",
      "\n",
      "--- Question 2 ---\n",
      "üîç Searching for: Which of those developments is most significant?\n",
      "Answer: Based on the search results, the most significant development highlighted is the decline in the number of children born to the average woman, which is noted as a pivotal change in the modern Western f...\n",
      "\n",
      "--- Question 3 ---\n",
      "üîç Searching for: How does it compare to last year?\n",
      "Answer: To compare data from this year to last year, there are several methods and tools you can use, depending on the context (e.g., sales data, personal metrics, or financial returns). Here‚Äôs a comprehensiv...\n"
     ]
    }
   ],
   "source": [
    "# LangChain: Trying to build Q&A with simple chains\n",
    "class LangChainQA:\n",
    "    def __init__(self):\n",
    "        # Chain 1: Search web\n",
    "        self.search_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"Search query for: {question}\"\n",
    "        )\n",
    "        \n",
    "        # Chain 2: Analyze results\n",
    "        self.analyze_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"search_results\"],\n",
    "            template=\"\"\"Question: {question}\n",
    "            \n",
    "Search Results:\n",
    "{search_results}\n",
    "\n",
    "Provide a comprehensive answer based on the search results:\"\"\"\n",
    "        )\n",
    "        self.analyze_chain = LLMChain(llm=llm, prompt=self.analyze_prompt)\n",
    "    \n",
    "    def answer_question(self, question: str):\n",
    "        \"\"\"Linear execution - search and answer\"\"\"\n",
    "        print(f\"üîç Searching for: {question}\")\n",
    "        \n",
    "        # Step 1: Search\n",
    "        try:\n",
    "            search_results = search_tool.invoke(question)\n",
    "            formatted_results = \"\\n\\n\".join([\n",
    "                f\"Source: {result['url']}\\n{result['content']}\"\n",
    "                for result in search_results\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Search failed: {e}\")\n",
    "            formatted_results = \"No search results available.\"\n",
    "        \n",
    "        # Step 2: Analyze and answer\n",
    "        answer = self.analyze_chain.run(question=question, search_results=formatted_results)\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def try_conversation(self, questions: List[str]):\n",
    "        \"\"\"Attempting conversation - loses context!\"\"\"\n",
    "        print(\"ü§ñ LangChain: Trying conversation (no memory!)...\")\n",
    "        \n",
    "        # ‚ùå Problem: Each question is independent!\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"\\n--- Question {i} ---\")\n",
    "            answer = self.answer_question(question)\n",
    "            print(f\"Answer: {answer[:200]}...\")\n",
    "            \n",
    "            # ‚ùå No context carried forward!\n",
    "            # Previous Q&A is completely lost\n",
    "\n",
    "# Test LangChain approach\n",
    "lc_qa = LangChainQA()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîó LANGCHAIN: Linear Q&A (No Context)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Single question works fine\n",
    "answer1 = lc_qa.answer_question(\"What are the latest developments in AI in 2024?\")\n",
    "print(f\"Answer: {answer1[:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîó LANGCHAIN: Conversation Attempt (Context Lost)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Follow-up questions lose context\n",
    "conversation = [\n",
    "    \"What are the latest developments in AI in 2024?\",\n",
    "    \"Which of those developments is most significant?\",  # ‚ùå No context about \"those\"\n",
    "    \"How does it compare to last year?\"  # ‚ùå No context about \"it\"\n",
    "]\n",
    "\n",
    "lc_qa.try_conversation(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üò´ Pain Points with LangChain for Conversational Q&A:\n",
    "\n",
    "1. **No Memory** - Each question is treated independently\n",
    "2. **Lost Context** - Follow-up questions don't make sense\n",
    "3. **No Conversation Flow** - Can't reference previous answers\n",
    "4. **Inefficient** - Re-searches the same topics repeatedly\n",
    "5. **Poor UX** - Users have to repeat context in every question\n",
    "6. **No Clarification** - Can't ask for more details or verification\n",
    "7. **Static Flow** - Can't adapt based on user engagement\n",
    "\n",
    "**ü§î The core issue**: Conversations require **state management** across turns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è B. LangGraph Approach (Stateful Conversations)\n",
    "\n",
    "## ‚úÖ The Solution: Conversational State with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üï∏Ô∏è LangGraph conversational Q&A workflow compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Define conversational state that persists across questions\n",
    "class ConversationState(TypedDict):\n",
    "    current_question: str\n",
    "    conversation_history: Annotated[list, operator.add]\n",
    "    search_results: list\n",
    "    current_answer: str\n",
    "    context_summary: str\n",
    "    user_intent: str  # \"new_question\", \"follow_up\", \"clarification\", \"done\"\n",
    "    search_needed: bool\n",
    "\n",
    "# Node functions - each manages part of the conversation\n",
    "def understand_question(state: ConversationState):\n",
    "    \"\"\"Analyze user question and determine intent\"\"\"\n",
    "    question = state[\"current_question\"]\n",
    "    history = state.get(\"conversation_history\", [])\n",
    "    \n",
    "    # Analyze if this is a follow-up or new question\n",
    "    if len(history) == 0:\n",
    "        intent = \"new_question\"\n",
    "        search_needed = True\n",
    "    else:\n",
    "        # Use LLM to determine intent based on conversation history\n",
    "        context = \"\\n\".join([msg for msg in history[-4:]])  # Last 2 Q&A pairs\n",
    "        \n",
    "        intent_prompt = f\"\"\"Based on this conversation:\n",
    "{context}\n",
    "\n",
    "New question: {question}\n",
    "\n",
    "Is this:\n",
    "- \"new_question\": Completely new topic needing fresh search\n",
    "- \"follow_up\": Related question that can use existing context  \n",
    "- \"clarification\": Asking for more details about previous answer\n",
    "\n",
    "Respond with just one word: new_question, follow_up, or clarification\"\"\"\n",
    "        \n",
    "        intent_response = llm.invoke([HumanMessage(content=intent_prompt)]).content.strip().lower()\n",
    "        intent = intent_response if intent_response in [\"new_question\", \"follow_up\", \"clarification\"] else \"follow_up\"\n",
    "        search_needed = intent in [\"new_question\", \"follow_up\"]\n",
    "    \n",
    "    return {\n",
    "        \"user_intent\": intent,\n",
    "        \"search_needed\": search_needed,\n",
    "        \"conversation_history\": [f\"üë§ User: {question}\"]\n",
    "    }\n",
    "\n",
    "def search_web_with_context(state: ConversationState):\n",
    "    \"\"\"Search web with conversation context\"\"\"\n",
    "    question = state[\"current_question\"]\n",
    "    intent = state[\"user_intent\"]\n",
    "    context_summary = state.get(\"context_summary\", \"\")\n",
    "    \n",
    "    # Enhance search query with context for follow-ups\n",
    "    if intent == \"follow_up\" and context_summary:\n",
    "        enhanced_query = f\"{question} {context_summary}\"\n",
    "    else:\n",
    "        enhanced_query = question\n",
    "    \n",
    "    print(f\"üîç Searching: {enhanced_query}\")\n",
    "    \n",
    "    try:\n",
    "        search_results = search_tool.invoke(enhanced_query)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Search failed: {e}\")\n",
    "        search_results = []\n",
    "    \n",
    "    return {\n",
    "        \"search_results\": search_results,\n",
    "        \"conversation_history\": [f\"üîç Searched for: {enhanced_query}\"]\n",
    "    }\n",
    "\n",
    "def generate_contextual_answer(state: ConversationState):\n",
    "    \"\"\"Generate answer using search results and conversation context\"\"\"\n",
    "    question = state[\"current_question\"]\n",
    "    search_results = state.get(\"search_results\", [])\n",
    "    history = state.get(\"conversation_history\", [])\n",
    "    intent = state[\"user_intent\"]\n",
    "    \n",
    "    # Format search results\n",
    "    if search_results:\n",
    "        formatted_results = \"\\n\\n\".join([\n",
    "            f\"Source {i+1}: {result['url']}\\n{result['content']}\"\n",
    "            for i, result in enumerate(search_results)\n",
    "        ])\n",
    "    else:\n",
    "        formatted_results = \"No search results available.\"\n",
    "    \n",
    "    # Include conversation context for follow-ups\n",
    "    recent_context = \"\\n\".join(history[-6:]) if len(history) > 1 else \"\"  # Last 3 exchanges\n",
    "    \n",
    "    if intent == \"clarification\":\n",
    "        prompt = f\"\"\"Based on our conversation context:\n",
    "{recent_context}\n",
    "\n",
    "User is asking for clarification: {question}\n",
    "\n",
    "Provide a helpful clarification based on the previous discussion and these search results:\n",
    "{formatted_results}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    elif intent == \"follow_up\":\n",
    "        prompt = f\"\"\"Conversation context:\n",
    "{recent_context}\n",
    "\n",
    "Follow-up question: {question}\n",
    "\n",
    "New search results:\n",
    "{formatted_results}\n",
    "\n",
    "Provide a comprehensive answer that builds on our previous discussion:\n",
    "\n",
    "Answer:\"\"\"\n",
    "    else:  # new_question\n",
    "        prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Search Results:\n",
    "{formatted_results}\n",
    "\n",
    "Provide a comprehensive, well-structured answer:\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    answer = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "    \n",
    "    # Update context summary for future questions\n",
    "    summary_prompt = f\"\"\"Summarize the key points from this Q&A for future context:\n",
    "Q: {question}\n",
    "A: {answer[:500]}...\n",
    "\n",
    "Brief summary (1-2 sentences):\"\"\"\n",
    "    \n",
    "    context_summary = llm.invoke([HumanMessage(content=summary_prompt)]).content\n",
    "    \n",
    "    return {\n",
    "        \"current_answer\": answer,\n",
    "        \"context_summary\": context_summary,\n",
    "        \"conversation_history\": [f\"ü§ñ Assistant: {answer[:100]}...\"]\n",
    "    }\n",
    "\n",
    "def check_if_search_needed(state: ConversationState):\n",
    "    \"\"\"Route based on whether search is needed\"\"\"\n",
    "    return \"search\" if state[\"search_needed\"] else \"answer\"\n",
    "\n",
    "# Build the conversational graph\n",
    "conversation_workflow = StateGraph(ConversationState)\n",
    "\n",
    "# Add nodes\n",
    "conversation_workflow.add_node(\"understand\", understand_question)\n",
    "conversation_workflow.add_node(\"search\", search_web_with_context)\n",
    "conversation_workflow.add_node(\"answer\", generate_contextual_answer)\n",
    "\n",
    "# Define the flow\n",
    "conversation_workflow.add_edge(START, \"understand\")\n",
    "\n",
    "# Conditional routing: search only if needed\n",
    "conversation_workflow.add_conditional_edges(\n",
    "    \"understand\",\n",
    "    check_if_search_needed,\n",
    "    {\n",
    "        \"search\": \"search\",\n",
    "        \"answer\": \"answer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "conversation_workflow.add_edge(\"search\", \"answer\")\n",
    "conversation_workflow.add_edge(\"answer\", END)\n",
    "\n",
    "# Compile the graph\n",
    "conversation_app = conversation_workflow.compile()\n",
    "\n",
    "print(\"üï∏Ô∏è LangGraph conversational Q&A workflow compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üï∏Ô∏è LANGGRAPH: Conversational Q&A with Memory\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "üë§ Question 1: What are the latest developments in AI in 2024?\n",
      "==================================================\n",
      "üîç Searching: What are the latest developments in AI in 2024?\n",
      "üß† Intent: new_question\n",
      "üîç Search needed: True\n",
      "üìù Answer: As of 2024, the landscape of artificial intelligence (AI) has seen significant advancements and developments across various domains. Here are the key trends and innovations shaping the AI field this year:\n",
      "\n",
      "### 1. **Miniaturization of AI Models**\n",
      "A notable trend in 2024 is the focus on creating smaller, more efficient AI models. Companies like Meta have introduced updates to their Llama AI model, w...\n",
      "\n",
      "üí≠ Context summary: In 2024, AI advancements include the miniaturization of models, exemplified by Meta's Llama AI, which is now faster and smaller, enhancing efficiency. This trend reflects a broader movement towards developing more compact and effective AI technologies across various sectors.\n",
      "\n",
      "==================================================\n",
      "üë§ Question 2: Which of those developments is most significant?\n",
      "==================================================\n",
      "üîç Searching: Which of those developments is most significant? In 2024, AI advancements include the miniaturization of models, exemplified by Meta's Llama AI, which is now faster and smaller, enhancing efficiency. This trend reflects a broader movement towards developing more compact and effective AI technologies across various sectors.\n",
      "üß† Intent: follow_up\n",
      "üîç Search needed: True\n",
      "üìù Answer: In 2024, several significant developments in artificial intelligence (AI) have emerged, each contributing to the evolution of technology and its applications across various sectors. Among these advancements, a few stand out as particularly impactful:\n",
      "\n",
      "1. **Miniaturization of AI Models**: The release of Meta's Llama 405B model exemplifies the trend towards smaller, more efficient AI systems. This m...\n",
      "\n",
      "üí≠ Context summary: In 2024, significant advancements in artificial intelligence include the miniaturization of AI models, exemplified by Meta's Llama 405B, which offers enhanced speed and efficiency while maintaining high performance, marking a pivotal shift in technology applications across various sectors.\n",
      "\n",
      "==================================================\n",
      "üë§ Question 3: How does it compare to last year?\n",
      "==================================================\n",
      "üîç Searching: How does it compare to last year? In 2024, significant advancements in artificial intelligence include the miniaturization of AI models, exemplified by Meta's Llama 405B, which offers enhanced speed and efficiency while maintaining high performance, marking a pivotal shift in technology applications across various sectors.\n",
      "üß† Intent: follow_up\n",
      "üîç Search needed: True\n",
      "üìù Answer: In 2024, the advancements in artificial intelligence (AI) have marked a significant evolution from the previous year, particularly in the areas of model miniaturization and performance enhancement. The introduction of Meta's Llama 405B model exemplifies this shift, showcasing a model that is not only faster and smaller but also maintains high performance levels. This contrasts with the development...\n",
      "\n",
      "üí≠ Context summary: In 2024, advancements in artificial intelligence have led to significant improvements in model miniaturization and performance, highlighted by Meta's Llama 405B model, which is faster and smaller while maintaining high performance. This marks a shift from 2023, where models were larger, less efficient, and required more computational resources.\n",
      "\n",
      "==================================================\n",
      "üë§ Question 4: Can you give me more specific examples?\n",
      "==================================================\n",
      "üîç Searching: Can you give me more specific examples? In 2024, advancements in artificial intelligence have led to significant improvements in model miniaturization and performance, highlighted by Meta's Llama 405B model, which is faster and smaller while maintaining high performance. This marks a shift from 2023, where models were larger, less efficient, and required more computational resources.\n",
      "üß† Intent: follow_up\n",
      "üîç Search needed: True\n",
      "üìù Answer: In 2024, the landscape of artificial intelligence (AI) has evolved significantly compared to 2023, marked by several groundbreaking advancements. Here are some specific examples that illustrate these developments:\n",
      "\n",
      "1. **Meta's Llama 405B Model**: One of the most notable advancements is the release of Meta's Llama 405B model, which boasts 405 billion parameters. This model demonstrates superior per...\n",
      "\n",
      "\n",
      "üìä Final Conversation Stats:\n",
      "  - Total questions: 4\n",
      "  - Conversation turns: 12\n",
      "  - Context maintained: ‚úÖ Throughout the conversation\n"
     ]
    }
   ],
   "source": [
    "# Demo the conversational Q&A workflow\n",
    "print(\"=\" * 60)\n",
    "print(\"üï∏Ô∏è LANGGRAPH: Conversational Q&A with Memory\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize conversation state\n",
    "conversation_state = {\n",
    "    \"current_question\": \"\",\n",
    "    \"conversation_history\": [],\n",
    "    \"search_results\": [],\n",
    "    \"current_answer\": \"\",\n",
    "    \"context_summary\": \"\",\n",
    "    \"user_intent\": \"\",\n",
    "    \"search_needed\": False\n",
    "}\n",
    "\n",
    "# Simulate a conversation\n",
    "questions = [\n",
    "    \"What are the latest developments in AI in 2024?\",\n",
    "    \"Which of those developments is most significant?\",  # ‚úÖ Has context!\n",
    "    \"How does it compare to last year?\",  # ‚úÖ Knows what \"it\" refers to!\n",
    "    \"Can you give me more specific examples?\"  # ‚úÖ Builds on previous answers!\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üë§ Question {i}: {question}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Update state with new question\n",
    "    conversation_state[\"current_question\"] = question\n",
    "    \n",
    "    # Run the workflow\n",
    "    result = conversation_app.invoke(conversation_state)\n",
    "    \n",
    "    # Update state with results\n",
    "    conversation_state.update(result)\n",
    "    \n",
    "    print(f\"üß† Intent: {result['user_intent']}\")\n",
    "    print(f\"üîç Search needed: {result['search_needed']}\")\n",
    "    print(f\"üìù Answer: {result['current_answer'][:400]}...\")\n",
    "    \n",
    "    if i < len(questions):\n",
    "        print(f\"\\nüí≠ Context summary: {result['context_summary']}\")\n",
    "\n",
    "print(f\"\\n\\nüìä Final Conversation Stats:\")\n",
    "print(f\"  - Total questions: {len(questions)}\")\n",
    "print(f\"  - Conversation turns: {len(conversation_state['conversation_history'])}\")\n",
    "print(f\"  - Context maintained: ‚úÖ Throughout the conversation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Benefits of LangGraph for Conversational Q&A:\n",
    "\n",
    "1. **Persistent Memory** - Maintains context across questions\n",
    "2. **Intent Recognition** - Understands follow-ups vs new topics\n",
    "3. **Efficient Search** - Only searches when needed\n",
    "4. **Contextual Answers** - References previous discussion\n",
    "5. **Natural Conversation** - Users can use pronouns and references\n",
    "6. **Adaptive Flow** - Different paths for different question types\n",
    "7. **State Management** - Automatically tracks conversation state\n",
    "8. **Scalable** - Easy to add new conversation features\n",
    "\n",
    "**üß† The key insight**: Conversations are **stateful workflows**, not linear chains!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Interactive Demo: Try It Yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Welcome to the LangGraph Q&A Assistant!\n",
      "Ask me anything, and I'll remember our conversation context.\n",
      "Try asking follow-up questions using 'it', 'that', 'those', etc.\n",
      "Type 'quit' to end the conversation.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ Question 1:  who is the president of the usa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Thinking...\n",
      "üîç Searching: who is the president of the usa\n",
      "\n",
      "ü§ñ Assistant: As of October 2023, the President of the United States is Donald J. Trump. He is the 47th president, having taken office on January 20, 2021. Trump, a member of the Republican Party, previously served as the 45th president from 2017 to 2021 before being succeeded by Joe Biden. His current term began after winning the 2020 presidential election.\n",
      "\n",
      "üí≠ (Intent: new_question, Search: Yes)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ Question 2:  today is june 2025. you should search the current president again\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Thinking...\n",
      "üîç Searching: today is june 2025. you should search the current president again\n",
      "\n",
      "ü§ñ Assistant: As of June 2025, the current President of the United States is Donald Trump. He has been actively involved in international diplomacy and domestic policy discussions, particularly regarding economic issues and foreign relations.\n",
      "\n",
      "1. **International Engagement**: Recently, President Trump participated in the G7 summit held in Kananaskis, Alberta, Canada, where he engaged with leaders from various countries, including Japan, Italy, France, Canada, the UK, Germany, and the European Union. His presence at the summit highlights his ongoing role in global diplomacy and international relations.\n",
      "\n",
      "2. **Domestic Policy and Economic Issues**: Domestically, Trump has been vocal about economic matters, particularly regarding the Federal Reserve's interest rate policies. Despite rising inflation and slowing GDP growth, he has faced challenges in influencing the Fed's decisions, which have remained steady at 4.25%‚Äì4.5%. His administration is focused on addressing economic concerns while also pushing back against what he describes as \"bloated federal spending.\"\n",
      "\n",
      "3. **Middle East Conflict**: The Trump administration is currently navigating a complex situation involving escalating tensions between Israel and Iran. While Trump has expressed a commitment to diplomatic solutions, there are indications that his administration may also consider military options as part of its strategy.\n",
      "\n",
      "In summary, Donald Trump is the President of the United States as of June 2025, and he continues to play a significant role in both domestic and international affairs, focusing on diplomacy, economic policy, and addressing conflicts in the Middle East.\n",
      "\n",
      "üí≠ (Intent: new_question, Search: Yes)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ Question 3:  who is the current vice president\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Thinking...\n",
      "üîç Searching: who is the current vice president As of June 2025, Donald Trump is the President of the United States, actively participating in international diplomacy and domestic policy, including recent involvement in the G7 summit in Kananaskis, Alberta, where he engaged with global leaders on economic and foreign relations issues.\n",
      "\n",
      "ü§ñ Assistant: As of June 2025, the current Vice President of the United States is Mike Pence. He has been serving alongside President Donald Trump during this administration. Pence has been involved in various domestic and international issues, including supporting Trump's policies and initiatives at events like the recent G7 summit in Kananaskis, Alberta.\n",
      "\n",
      "üí≠ (Intent: follow_up, Search: Yes)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ Question 4:  you are wrong, it is not mike pence. search again\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Thinking...\n",
      "üîç Searching: you are wrong, it is not mike pence. search again As of June 2025, the Vice President of the United States is Mike Pence, who has been serving alongside President Donald Trump and has participated in various domestic and international matters, including the recent G7 summit in Kananaskis, Alberta.\n",
      "\n",
      "ü§ñ Assistant: As of June 2025, the Vice President of the United States is not Mike Pence. In fact, Mike Pence served as Vice President under Donald Trump from 2017 to 2021. Following the 2020 election, Kamala Harris became Vice President alongside President Joe Biden. However, the political landscape has shifted significantly since then, and as of June 2025, it appears that there has been a change in administration or leadership roles that I may not have the latest information on.\n",
      "\n",
      "If you are looking for the most current and accurate information regarding the Vice President, I recommend checking reliable news sources or official government announcements to confirm who is currently serving in that role.\n",
      "\n",
      "üí≠ (Intent: follow_up, Search: Yes)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ Question 5:  yes, search the most current and accurate information regarding the VP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Thinking...\n",
      "üîç Searching: yes, search the most current and accurate information regarding the VP\n",
      "\n",
      "ü§ñ Assistant: As of now, the current Vice President of the United States is **James David (JD) Vance**, who was sworn in on **January 20, 2025**. He serves alongside President Donald Trump, who is currently in office. This information can be confirmed through official government sources, such as the White House website.\n",
      "\n",
      "### Background on JD Vance\n",
      "- **Political Affiliation**: JD Vance is a member of the Republican Party.\n",
      "- **Previous Experience**: Before becoming Vice President, Vance was known for his role as an author and venture capitalist, gaining national attention for his memoir \"Hillbilly Elegy,\" which discusses his upbringing in Ohio and the socio-economic issues facing rural America.\n",
      "\n",
      "### Responsibilities of the Vice President\n",
      "The Vice President has several key responsibilities, including:\n",
      "- Assisting the President in executing the administration's agenda.\n",
      "- Presiding over the Senate and casting tie-breaking votes when necessary.\n",
      "- Representing the United States at official functions and diplomatic events.\n",
      "\n",
      "### Current Political Context\n",
      "The political landscape is dynamic, with ongoing discussions about various issues such as immigration, healthcare, and economic policy. The Vice President plays a crucial role in shaping and supporting the administration's policies on these matters.\n",
      "\n",
      "For more detailed information about the Vice President and his role, you can visit the official [White House website](https://www.whitehouse.gov/administration/jd-vance/) or the [USA.gov page on Presidents](https://www.usa.gov/presidents).\n",
      "\n",
      "üí≠ (Intent: new_question, Search: Yes)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ Question 6:  done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Thinking...\n",
      "üîç Searching: done\n",
      "\n",
      "ü§ñ Assistant: The term \"done\" can refer to several contexts, depending on its usage. Here‚Äôs a comprehensive overview based on the search results:\n",
      "\n",
      "1. **Definition and Usage**:\n",
      "   - According to Merriam-Webster, \"done\" is the past participle of the verb \"do,\" indicating that an action has been completed. For example, one might say, \"One more question and we're done,\" signifying that the task is finished (Source 2).\n",
      "   - Cambridge Dictionary also highlights that if something is \"done,\" it means it is finished or completed (Source 3).\n",
      "\n",
      "2. **Cultural References**:\n",
      "   - The term is also used in popular culture, such as in music. For instance, Chris Janson's song \"Done\" expresses feelings associated with love and relationships, emphasizing the emotional weight of being \"done\" with certain experiences (Source 4).\n",
      "\n",
      "3. **Health and Wellness Applications**:\n",
      "   - The term \"done\" is also associated with health and wellness applications, such as the Done app, which helps users create healthy routines by setting goals and tracking progress. This app motivates users to maintain their streaks and chains of healthy habits (Source 5).\n",
      "\n",
      "4. **Service Offerings**:\n",
      "   - Additionally, \"done\" is featured in a service context, such as Done First, which offers personalized treatment plans and online consultations for a monthly fee. This service highlights the completion of mental health or wellness goals through structured support (Source 1).\n",
      "\n",
      "In summary, \"done\" can refer to the completion of tasks, emotional states in relationships, or services aimed at improving personal well-being. Its meaning can vary widely based on context, from everyday language to specific applications in health and wellness.\n",
      "\n",
      "üí≠ (Intent: new_question, Search: Yes)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def interactive_qa():\n",
    "    \"\"\"Interactive Q&A session - try asking follow-up questions!\"\"\"\n",
    "    print(\"ü§ñ Welcome to the LangGraph Q&A Assistant!\")\n",
    "    print(\"Ask me anything, and I'll remember our conversation context.\")\n",
    "    print(\"Try asking follow-up questions using 'it', 'that', 'those', etc.\")\n",
    "    print(\"Type 'quit' to end the conversation.\\n\")\n",
    "    \n",
    "    # Initialize conversation state\n",
    "    state = {\n",
    "        \"current_question\": \"\",\n",
    "        \"conversation_history\": [],\n",
    "        \"search_results\": [],\n",
    "        \"current_answer\": \"\",\n",
    "        \"context_summary\": \"\",\n",
    "        \"user_intent\": \"\",\n",
    "        \"search_needed\": False\n",
    "    }\n",
    "    \n",
    "    question_count = 0\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_question = input(f\"üë§ Question {question_count + 1}: \").strip()\n",
    "        \n",
    "        if user_question.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"üëã Thanks for chatting! Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not user_question:\n",
    "            print(\"Please ask a question!\")\n",
    "            continue\n",
    "        \n",
    "        question_count += 1\n",
    "        \n",
    "        # Update state\n",
    "        state[\"current_question\"] = user_question\n",
    "        \n",
    "        print(\"\\nü§ñ Thinking...\")\n",
    "        \n",
    "        try:\n",
    "            # Run the workflow\n",
    "            result = conversation_app.invoke(state)\n",
    "            \n",
    "            # Update state with results\n",
    "            state.update(result)\n",
    "            \n",
    "            # Show the answer\n",
    "            print(f\"\\nü§ñ Assistant: {result['current_answer']}\")\n",
    "            print(f\"\\nüí≠ (Intent: {result['user_intent']}, Search: {'Yes' if result['search_needed'] else 'No'})\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            print(\"Let's try a different question!\")\n",
    "\n",
    "# Uncomment the line below to start an interactive session!\n",
    "interactive_qa()\n",
    "\n",
    "print(\"üí° Tip: Uncomment the line above to try the interactive Q&A!\")\n",
    "print(\"Sample conversation flow:\")\n",
    "print(\"1. 'What is machine learning?'\")\n",
    "print(\"2. 'How does it differ from traditional programming?'\")\n",
    "print(\"3. 'Can you give me a practical example?'\")\n",
    "print(\"4. 'What are the main challenges with that approach?'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Side-by-Side Comparison\n",
    "\n",
    "## üîó LangChain Approach:\n",
    "```python\n",
    "# ‚ùå Each question is independent\n",
    "def answer_question(question):\n",
    "    results = search_tool.invoke(question)\n",
    "    answer = analyze_chain.run(question=question, results=results)\n",
    "    return answer  # Context lost!\n",
    "\n",
    "# Follow-up fails:\n",
    "# Q1: \"What is AI?\"\n",
    "# Q2: \"How does it work?\" ‚ùå No context about \"it\"\n",
    "```\n",
    "\n",
    "## üï∏Ô∏è LangGraph Approach:\n",
    "```python\n",
    "# ‚úÖ State persists across questions\n",
    "class ConversationState(TypedDict):\n",
    "    conversation_history: list\n",
    "    context_summary: str\n",
    "    user_intent: str\n",
    "\n",
    "# Smart routing based on intent:\n",
    "workflow.add_conditional_edges(\n",
    "    \"understand\",\n",
    "    check_if_search_needed,\n",
    "    {\"search\": \"search\", \"answer\": \"answer\"}\n",
    ")\n",
    "# Context maintained automatically!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ When to Use What?\n",
    "\n",
    "## üîó Use **LangChain** for:\n",
    "- ‚úÖ **One-shot Q&A** (single question, single answer)\n",
    "- ‚úÖ **Batch processing** (many independent questions)\n",
    "- ‚úÖ **Simple search-and-summarize** workflows\n",
    "- ‚úÖ **Quick prototypes** without conversation needs\n",
    "\n",
    "## üï∏Ô∏è Use **LangGraph** for:\n",
    "- ‚úÖ **Conversational Q&A** with follow-up questions\n",
    "- ‚úÖ **Context-aware interactions** (\"it\", \"that\", \"those\")\n",
    "- ‚úÖ **Multi-turn conversations** with memory\n",
    "- ‚úÖ **Adaptive search** (search only when needed)\n",
    "- ‚úÖ **User intent recognition** (new vs follow-up questions)\n",
    "- ‚úÖ **State management** across conversation turns\n",
    "\n",
    "## üí° Real-World Decision Matrix:\n",
    "\n",
    "| Scenario | Tool | Reason |\n",
    "|----------|------|--------|\n",
    "| FAQ Bot (isolated questions) | **LangChain** | No conversation context needed |\n",
    "| Research Assistant | **LangGraph** | Needs follow-up and clarification |\n",
    "| Document Q&A (one-off) | **LangChain** | Simple question about static content |\n",
    "| Chatbot Support | **LangGraph** | Conversational, context-aware |\n",
    "| Batch Q&A Processing | **LangChain** | Independent questions, no memory |\n",
    "| Interactive Learning Tutor | **LangGraph** | Progressive conversation with context |\n",
    "\n",
    "## üöÄ Migration Path:\n",
    "```\n",
    "LangChain Single Q&A ‚Üí User wants follow-ups ‚Üí LangGraph Conversation\n",
    "        ‚Üì                        ‚Üì                        ‚Üì\n",
    "   \"What is X?\"          \"Tell me more about Y\"    \"How does it relate to Z?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Key Takeaways\n",
    "\n",
    "## üîç **The Fundamental Difference**:\n",
    "- **LangChain**: \"Answer this question\" (stateless)\n",
    "- **LangGraph**: \"Have a conversation with me\" (stateful)\n",
    "\n",
    "## üß† **Mental Models**:\n",
    "- **LangChain**: Library reference lookup - each query is independent\n",
    "- **LangGraph**: Human conversation - context matters, memory persists\n",
    "\n",
    "## üîÑ **State Management**:\n",
    "- **LangChain**: No memory ‚Üí \"What is machine learning? How does it work?\" (confusing)\n",
    "- **LangGraph**: Persistent state ‚Üí \"What is ML? How does **it** work?\" (natural)\n",
    "\n",
    "## üéØ **The Decision Questions**:\n",
    "1. **Will users ask follow-up questions?** ‚Üí LangGraph\n",
    "2. **Do they need to reference previous answers?** ‚Üí LangGraph\n",
    "3. **Is this a one-shot Q&A?** ‚Üí LangChain\n",
    "4. **Do users say \"it\", \"that\", \"those\"?** ‚Üí LangGraph\n",
    "\n",
    "## üí° **Pro Tips**:\n",
    "- **Start with LangChain** for MVP/prototype\n",
    "- **Upgrade to LangGraph** when users ask \"Can you elaborate?\"\n",
    "- **LangGraph shines** when conversation context matters\n",
    "- **Both tools** can work together in larger systems\n",
    "\n",
    "## üöÄ **Next Steps**:\n",
    "1. **Try the interactive demo** above\n",
    "2. **Experiment** with different question types\n",
    "3. **Notice** how context affects answer quality\n",
    "4. **Build** your own conversational workflows!\n",
    "\n",
    "**Remember**: The best tool depends on your users' conversation patterns! üó£Ô∏è"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
