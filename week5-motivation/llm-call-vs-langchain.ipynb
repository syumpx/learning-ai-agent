{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/session-4/why-langchain-and-langgraph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-why-langchain-and-langgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why LangChain and LangGraph?\n",
    "\n",
    "## üéØ Learning Goals\n",
    "\n",
    "This notebook demonstrates the **evolution from raw LLM calls to sophisticated AI workflows**:\n",
    "\n",
    "1. **Plain LLM Calls** ‚Üí **LangChain**: See how LangChain simplifies common patterns\n",
    "2. **LangChain** ‚Üí **LangGraph**: Understand when you need stateful, multi-step workflows\n",
    "3. **Real-world Examples**: Document Q&A, Research Assistant, and Multi-Agent Systems\n",
    "\n",
    "## üß† The Problem\n",
    "\n",
    "As AI applications grow more complex, developers face increasing challenges:\n",
    "\n",
    "- **Repetitive boilerplate code** for common patterns\n",
    "- **No standardization** across teams and projects\n",
    "- **Complex state management** for multi-step workflows\n",
    "- **Difficulty in testing, debugging, and monitoring** AI pipelines\n",
    "\n",
    "Let's see how LangChain and LangGraph solve these problems step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python wikipedia faiss-cpu scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded:\n",
      "OPENAI_API_KEY: ‚úì Set\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the keys are loaded (optional - remove in production)\n",
    "print(\"‚úÖ Environment variables loaded:\")\n",
    "print(f\"OPENAI_API_KEY: {'‚úì Set' if os.environ.get('OPENAI_API_KEY') else '‚úó Missing'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import openai\n",
    "\n",
    "# We'll use both direct OpenAI client and LangChain for comparison\n",
    "openai_client = openai.OpenAI()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Example 1: Plain LLM Call vs LangChain\n",
    "\n",
    "## üß† Task: Document Q&A\n",
    "\"Answer a question about a document uploaded by the user.\"\n",
    "\n",
    "This is one of the most common AI use cases. Let's see how the approach evolves from naive implementation to LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è A. Plain LLM Call (Naive Implementation)\n",
    "\n",
    "### üò´ What you have to do manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample documents loaded:\n",
      "1. LangChain is a framework for developing applicatio...\n",
      "2. LangGraph is a library for building stateful, mult...\n",
      "3. Vector databases store high-dimensional vectors an...\n",
      "4. RAG combines the power of retrieval systems with g...\n"
     ]
    }
   ],
   "source": [
    "# First, let's create some sample documents\n",
    "sample_documents = [\n",
    "    \"LangChain is a framework for developing applications powered by language models. It enables applications that are data-aware and agentic.\",\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs. It extends LangChain with the ability to coordinate multiple chains across multiple steps.\",\n",
    "    \"Vector databases store high-dimensional vectors and allow for efficient similarity search. They are essential for RAG (Retrieval Augmented Generation) applications.\",\n",
    "    \"RAG combines the power of retrieval systems with generative models. It retrieves relevant documents and uses them as context for generating responses.\"\n",
    "]\n",
    "\n",
    "print(\"Sample documents loaded:\")\n",
    "for i, doc in enumerate(sample_documents, 1):\n",
    "    print(f\"{i}. {doc[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Implementation Result:\n",
      "LangGraph is a library designed for building stateful, multi-actor applications using large language models (LLMs). It enhances LangChain by enabling the coordination of multiple chains across various steps.\n"
     ]
    }
   ],
   "source": [
    "# Naive implementation - lots of manual work!\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class NaiveDocumentQA:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        # Manual embedding with TF-IDF (not even proper embeddings!)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.doc_vectors = self.vectorizer.fit_transform(documents)\n",
    "    \n",
    "    def answer_question(self, question):\n",
    "        # Manual retrieval\n",
    "        question_vector = self.vectorizer.transform([question])\n",
    "        similarities = cosine_similarity(question_vector, self.doc_vectors)\n",
    "        best_doc_idx = np.argmax(similarities)\n",
    "        \n",
    "        # Manual context formatting\n",
    "        context = self.documents[best_doc_idx]\n",
    "        \n",
    "        # Manual prompt construction\n",
    "        prompt = f\"\"\"Answer the question based on the context below.\n",
    "        \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Manual LLM call\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Test the naive implementation\n",
    "naive_qa = NaiveDocumentQA(sample_documents)\n",
    "result = naive_qa.answer_question(\"What is LangGraph?\")\n",
    "print(\"Naive Implementation Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üò´ Pain Points with Naive Implementation:\n",
    "\n",
    "1. **Manual glue logic** - You write all the embedding, retrieval, and formatting code\n",
    "2. **No prompt templating** - Hard-coded strings everywhere\n",
    "3. **Poor embeddings** - Using TF-IDF instead of proper semantic embeddings\n",
    "4. **No memory, retry, logging** - Zero observability or error handling\n",
    "5. **Not modular** - Hard to swap components or test individual parts\n",
    "6. **No optimization** - No chunk sizing, overlap, or retrieval tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è B. LangChain Version\n",
    "\n",
    "### ‚úÖ Let's see how LangChain simplifies this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/q4bp4bs504v2x945rq65zhsw0000gn/T/ipykernel_64542/3749501469.py:21: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain.run(\"What is LangGraph?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Implementation Result:\n",
      "LangGraph is a library for building stateful, multi-actor applications with large language models (LLMs). It extends LangChain by enabling the coordination of multiple chains across multiple steps.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Convert strings to Document objects\n",
    "docs = [Document(page_content=doc) for doc in sample_documents]\n",
    "\n",
    "# Create embeddings and vector store - one line!\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Create QA chain - just a few lines!\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Ask the same question\n",
    "result = qa_chain.run(\"What is LangGraph?\")\n",
    "print(\"LangChain Implementation Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Benefits of LangChain:\n",
    "\n",
    "1. **Powerful Abstractions**: `RetrievalQA`, `Retriever`, `VectorStore`\n",
    "2. **Modular Design**: Easy to swap LLMs, embeddings, or vector stores\n",
    "3. **Built-in Best Practices**: Proper embeddings, prompt templates, error handling\n",
    "4. **Observability**: Built-in logging, tracing, and evaluation tools\n",
    "5. **Community**: Pre-built integrations with 100+ services\n",
    "6. **Maintenance**: Framework handles updates and optimizations\n",
    "\n",
    "**üß† Why it's intuitive:** Everyone has tried to make LLMs answer questions about docs. LangChain shows how 2‚Äì3 lines replace a full pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Summary: The Evolution\n",
    "\n",
    "## üìà Complexity vs Capability\n",
    "\n",
    "| Approach | Complexity | Capabilities | Best For |\n",
    "|----------|------------|-------------|----------|\n",
    "| **Plain LLM** | üü¢ Low | üî¥ Limited | Simple, one-off queries |\n",
    "| **LangChain** | üü° Medium | üü° Good | Standard patterns, moderate complexity |\n",
    "| **LangGraph** | üî¥ High | üü¢ Excellent | Complex workflows, multi-step processes |\n",
    "\n",
    "## üöÄ When to Use What?\n",
    "\n",
    "### Use **Plain LLM Calls** when:\n",
    "- Simple, one-time queries\n",
    "- Prototyping and experimentation\n",
    "- Full control over every aspect\n",
    "- Minimal dependencies\n",
    "\n",
    "### Use **LangChain** when:\n",
    "- Document Q&A, summarization, translation\n",
    "- Need standard patterns and abstractions\n",
    "- Want community integrations\n",
    "- Linear workflows are sufficient\n",
    "\n",
    "### Use **LangGraph** when:\n",
    "- Multi-step, conditional workflows\n",
    "- Need state management between steps\n",
    "- Human-in-the-loop requirements\n",
    "- Multiple agents working together\n",
    "- Error recovery and retry logic\n",
    "- Complex business processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Next Steps\n",
    "\n",
    "## Try It Yourself:\n",
    "\n",
    "1. **Start with LangChain**: Build a document Q&A system for your domain\n",
    "2. **Add Complexity**: When you need multi-step workflows, explore LangGraph\n",
    "3. **Production**: Add monitoring, error handling, and human oversight\n",
    "\n",
    "## Resources:\n",
    "\n",
    "- [LangChain Documentation](https://docs.langchain.com/)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangChain Academy](https://academy.langchain.com/)\n",
    "- [Example Applications](https://github.com/langchain-ai/langchain/tree/master/templates)\n",
    "\n",
    "**Remember**: Choose the right tool for the job. Sometimes a simple LLM call is all you need!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
