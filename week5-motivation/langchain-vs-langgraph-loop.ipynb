{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/session-4/loops-with-langchain-and-langgraph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-loops-with-langchain-and-langgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loops with LangChain vs LangGraph\n",
    "\n",
    "## üîÑ The Challenge: Iterative Information Gathering\n",
    "\n",
    "This notebook demonstrates a fundamental limitation of **LangChain** and shows how **LangGraph** naturally handles **loops and iterations**.\n",
    "\n",
    "### üéØ Real-World Scenario: Iterative Research Assistant\n",
    "**Build an AI that conducts thorough research by:**\n",
    "\n",
    "1. üìù **Taking a user's research question**\n",
    "2. üîç **Searching the web for initial information**\n",
    "3. ü§î **Analyzing if the information is sufficient**\n",
    "4. üîÑ **If not sufficient: generating follow-up search queries and repeating**\n",
    "5. ‚úÖ **When sufficient: providing comprehensive final answer**\n",
    "6. üë§ **Asking user if they want more detail on any aspect**\n",
    "7. üîÅ **Looping back to gather more specific information if requested**\n",
    "\n",
    "### üí≠ Why This Matters:\n",
    "Real research is **iterative**:\n",
    "- Initial search reveals knowledge gaps\n",
    "- New questions emerge from partial answers\n",
    "- Users want to drill down into specific aspects\n",
    "- Quality research requires **multiple search-analyze cycles**\n",
    "\n",
    "Let's see how each framework handles this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the keys are loaded (optional - remove in production)\n",
    "print(\"‚úÖ Environment variables loaded:\")\n",
    "print(f\"OPENAI_API_KEY: {'‚úì Set' if os.environ.get('OPENAI_API_KEY') else '‚úó Missing'}\")\n",
    "print(f\"TAVILY_API_KEY: {'‚úì Set' if os.environ.get('TAVILY_API_KEY') else '‚úó Missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing import Dict, List, Annotated\n",
    "import operator\n",
    "import time\n",
    "\n",
    "# Initialize LLM and search tool\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "search_tool = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó A. LangChain Approach: No Native Loop Support\n",
    "\n",
    "## üò´ The Problem: Linear Chains Can't Loop Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain: Attempting to create iterative research (doesn't work well)\n",
    "class LangChainIterativeResearch:\n",
    "    def __init__(self):\n",
    "        # Chain 1: Initial search\n",
    "        self.search_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"Generate a search query for: {question}\"\n",
    "        )\n",
    "        \n",
    "        # Chain 2: Analyze completeness\n",
    "        self.analysis_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"search_results\"],\n",
    "            template=\"\"\"Question: {question}\n",
    "Search Results: {search_results}\n",
    "\n",
    "Is this information sufficient to fully answer the question? \n",
    "If not, what specific aspects need more research?\n",
    "\n",
    "Respond with:\n",
    "SUFFICIENT: Yes/No\n",
    "GAPS: [list any information gaps]\n",
    "NEXT_QUERY: [suggest next search query if not sufficient]\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.analysis_chain = LLMChain(llm=llm, prompt=self.analysis_prompt)\n",
    "        \n",
    "        # Chain 3: Final answer\n",
    "        self.answer_prompt = PromptTemplate(\n",
    "            input_variables=[\"question\", \"all_results\"],\n",
    "            template=\"\"\"Question: {question}\n",
    "All Research Results: {all_results}\n",
    "\n",
    "Provide a comprehensive answer based on all the research:\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.answer_chain = LLMChain(llm=llm, prompt=self.answer_prompt)\n",
    "    \n",
    "    def research_with_manual_loops(self, question: str, max_iterations: int = 10):\n",
    "        \"\"\"Attempt iterative research with manual while loops - HACKY!\"\"\"\n",
    "        print(f\"üîç Starting research on: {question}\")\n",
    "        \n",
    "        all_results = []\n",
    "        current_query = question\n",
    "        iteration = 0\n",
    "        \n",
    "        # ‚ùå PROBLEM: Manual while loop - not part of LangChain's design!\n",
    "        while iteration < max_iterations: #max_retry\n",
    "            iteration += 1\n",
    "            print(f\"\\n--- Iteration {iteration} ---\")\n",
    "            print(f\"üîç Searching: {current_query}\")\n",
    "            \n",
    "            # Search\n",
    "            try:\n",
    "                search_results = search_tool.invoke(current_query)\n",
    "                formatted_results = \"\\n\\n\".join([\n",
    "                    f\"Source: {result['url']}\\n{result['content']}\"\n",
    "                    for result in search_results\n",
    "                ])\n",
    "                all_results.append(formatted_results)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Search failed: {e}\")\n",
    "                break\n",
    "            \n",
    "            # ‚ùå PROBLEM: Chain can't decide to loop - we force it with external logic\n",
    "            analysis = self.analysis_chain.run(\n",
    "                question=question, \n",
    "                search_results=formatted_results\n",
    "            )\n",
    "            \n",
    "            print(f\"üìä Analysis: {analysis[:200]}...\")\n",
    "            \n",
    "            # ‚ùå PROBLEM: Crude parsing to determine if we should continue\n",
    "            if \"SUFFICIENT: Yes\" in analysis or \"Yes\" in analysis.split('\\n')[0]:\n",
    "                print(\"‚úÖ Research deemed sufficient\")\n",
    "                break\n",
    "            \n",
    "            # ‚ùå PROBLEM: Manually extract next query - very brittle!\n",
    "            lines = analysis.split('\\n')\n",
    "            next_query = current_query  # Default fallback\n",
    "            for line in lines:\n",
    "                if \"NEXT_QUERY:\" in line:\n",
    "                    next_query = line.split(\"NEXT_QUERY:\")[1].strip()\n",
    "                    break\n",
    "            \n",
    "            current_query = next_query\n",
    "            \n",
    "            # ‚ùå PROBLEM: Manual delay to avoid hitting rate limits\n",
    "            time.sleep(1)\n",
    "        \n",
    "        # Final answer\n",
    "        all_research = \"\\n\\n=== RESEARCH ROUND ===\\n\\n\".join(all_results)\n",
    "        final_answer = self.answer_chain.run(question=question, all_results=all_research)\n",
    "        \n",
    "        return final_answer, iteration\n",
    "    \n",
    "    def attempt_user_interaction(self):\n",
    "        \"\"\"Try to create interactive research - shows LangChain's limitations\"\"\"\n",
    "        print(\"ü§ñ LangChain Interactive Research (Attempt)\")\n",
    "        print(\"‚ùå Note: This is hacky and not how LangChain is designed to work!\\n\")\n",
    "        \n",
    "        # Get user question\n",
    "        user_question = input(\"üë§ What would you like to research? \")\n",
    "        \n",
    "        if not user_question.strip():\n",
    "            print(\"Please provide a question!\")\n",
    "            return\n",
    "        \n",
    "        # Do initial research\n",
    "        answer, iterations = self.research_with_manual_loops(user_question)\n",
    "        \n",
    "        print(f\"\\nüéØ Final Answer ({iterations} iterations):\")\n",
    "        print(answer)\n",
    "        \n",
    "        # ‚ùå PROBLEM: Can't easily loop back for follow-up questions\n",
    "        # Would need to restart the entire process!\n",
    "        print(\"\\n‚ùå LangChain Limitation: Follow-up questions require starting over!\")\n",
    "\n",
    "# Demo LangChain limitations\n",
    "lc_research = LangChainIterativeResearch()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîó LANGCHAIN: Attempting Iterative Research\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Uncomment to try interactive mode:\n",
    "# lc_research.attempt_user_interaction()\n",
    "\n",
    "# For demo purposes, let's show with a hardcoded example:\n",
    "demo_question = \"Who is the president of usa\"\n",
    "demo_answer, demo_iterations = lc_research.research_with_manual_loops(demo_question, max_iterations=10)\n",
    "print(f\"\\nüéØ Demo Result: {demo_answer[:300]}...\")\n",
    "print(f\"üìä Required {demo_iterations} manual iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üò´ Problems with LangChain for Loops:\n",
    "\n",
    "1. **No Native Loop Support** - Chains are designed to be linear\n",
    "2. **Manual While Loops** - External control logic required (hacky!)\n",
    "3. **Brittle Condition Checking** - Text parsing to determine if loop should continue\n",
    "4. **No State Management** - Can't track iteration state naturally\n",
    "5. **Poor Error Handling** - If one iteration fails, hard to recover\n",
    "6. **No Conditional Routing** - Can't dynamically choose next steps\n",
    "7. **Restart Required** - Follow-up questions need complete restart\n",
    "8. **Complex Control Flow** - Business logic mixed with framework limitations\n",
    "\n",
    "**ü§î The core issue**: LangChain chains are **directed acyclic graphs** (DAGs) - no cycles allowed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è B. LangGraph Approach: Loops are Natural\n",
    "\n",
    "## ‚úÖ The Solution: Built-in Loop Support with State Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Define research state that persists across loop iterations\n",
    "class ResearchState(TypedDict):\n",
    "    original_question: str\n",
    "    current_query: str\n",
    "    search_results: Annotated[list, operator.add]\n",
    "    analysis_history: Annotated[list, operator.add]\n",
    "    iteration_count: int\n",
    "    research_complete: bool\n",
    "    information_gaps: List[str]\n",
    "    final_answer: str\n",
    "    user_satisfied: bool\n",
    "    follow_up_requested: str\n",
    "\n",
    "# Node functions for the research workflow\n",
    "def search_web(state: ResearchState):\n",
    "    \"\"\"Search the web for current query\"\"\"\n",
    "    query = state[\"current_query\"]\n",
    "    iteration = state[\"iteration_count\"]\n",
    "    \n",
    "    print(f\"üîç Iteration {iteration}: Searching for '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        search_results = search_tool.invoke(query)\n",
    "        formatted_results = {\n",
    "            \"query\": query,\n",
    "            \"results\": search_results,\n",
    "            \"formatted\": \"\\n\\n\".join([\n",
    "                f\"Source: {result['url']}\\n{result['content']}\"\n",
    "                for result in search_results\n",
    "            ])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Search failed: {e}\")\n",
    "        formatted_results = {\n",
    "            \"query\": query,\n",
    "            \"results\": [],\n",
    "            \"formatted\": f\"Search failed: {e}\"\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"search_results\": [formatted_results]\n",
    "    }\n",
    "\n",
    "def analyze_completeness(state: ResearchState):\n",
    "    \"\"\"Analyze if research is complete or needs more iterations\"\"\"\n",
    "    question = state[\"original_question\"]\n",
    "    all_results = state[\"search_results\"]\n",
    "    iteration = state[\"iteration_count\"]\n",
    "    \n",
    "    # Combine all research so far\n",
    "    combined_research = \"\\n\\n--- RESEARCH ROUND ---\\n\\n\".join([\n",
    "        result[\"formatted\"] for result in all_results\n",
    "    ])\n",
    "    \n",
    "    analysis_prompt = f\"\"\"Original Question: {question}\n",
    "\n",
    "Research Completed So Far:\n",
    "{combined_research}\n",
    "\n",
    "Analysis Task:\n",
    "1. Is this information sufficient to provide a comprehensive answer?\n",
    "2. What specific information gaps remain?\n",
    "3. If more research needed, what should be the next search query?\n",
    "\n",
    "Respond in this format:\n",
    "COMPLETE: true/false\n",
    "GAPS: [list any remaining information gaps]\n",
    "NEXT_QUERY: [specific search query for next iteration]\n",
    "REASONING: [why more research is or isn't needed]\"\"\"\n",
    "    \n",
    "    analysis = llm.invoke([HumanMessage(content=analysis_prompt)]).content\n",
    "    \n",
    "    # Parse analysis (more robust than LangChain version)\n",
    "    complete = \"COMPLETE: true\" in analysis.lower() or iteration >= 3  # Max 3 iterations\n",
    "    \n",
    "    # Extract information gaps\n",
    "    gaps = []\n",
    "    lines = analysis.split('\\n')\n",
    "    for line in lines:\n",
    "        if line.startswith(\"GAPS:\"):\n",
    "            gaps_text = line.replace(\"GAPS:\", \"\").strip()\n",
    "            if gaps_text and gaps_text != \"None\":\n",
    "                gaps = [gap.strip() for gap in gaps_text.split(\",\")]\n",
    "    \n",
    "    # Extract next query\n",
    "    next_query = state[\"current_query\"]  # Default fallback\n",
    "    for line in lines:\n",
    "        if line.startswith(\"NEXT_QUERY:\"):\n",
    "            next_query = line.replace(\"NEXT_QUERY:\", \"\").strip()\n",
    "            break\n",
    "    \n",
    "    print(f\"üìä Analysis complete: Research {'finished' if complete else 'continuing'}\")\n",
    "    if not complete:\n",
    "        print(f\"üéØ Next query: {next_query}\")\n",
    "    \n",
    "    return {\n",
    "        \"analysis_history\": [{\n",
    "            \"iteration\": iteration,\n",
    "            \"analysis\": analysis,\n",
    "            \"complete\": complete\n",
    "        }],\n",
    "        \"research_complete\": complete,\n",
    "        \"information_gaps\": gaps,\n",
    "        \"current_query\": next_query,\n",
    "        \"iteration_count\": iteration + 1\n",
    "    }\n",
    "\n",
    "def generate_final_answer(state: ResearchState):\n",
    "    \"\"\"Generate comprehensive final answer from all research\"\"\"\n",
    "    question = state[\"original_question\"]\n",
    "    all_results = state[\"search_results\"]\n",
    "    \n",
    "    # Combine all research\n",
    "    combined_research = \"\\n\\n--- RESEARCH ROUND ---\\n\\n\".join([\n",
    "        f\"Query: {result['query']}\\n{result['formatted']}\"\n",
    "        for result in all_results\n",
    "    ])\n",
    "    \n",
    "    final_prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Complete Research Results:\n",
    "{combined_research}\n",
    "\n",
    "Please provide a comprehensive, well-structured answer that synthesizes all the research findings. \n",
    "Include key points, examples, and cite sources where relevant.\"\"\"\n",
    "    \n",
    "    final_answer = llm.invoke([HumanMessage(content=final_prompt)]).content\n",
    "    \n",
    "    print(\"‚úÖ Final answer generated\")\n",
    "    \n",
    "    return {\n",
    "        \"final_answer\": final_answer\n",
    "    }\n",
    "\n",
    "def ask_user_satisfaction(state: ResearchState):\n",
    "    \"\"\"Check if user wants more details on any aspect\"\"\"\n",
    "    answer = state[\"final_answer\"]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ RESEARCH COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(answer)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    follow_up = input(\"\\nüë§ Would you like more detail on any specific aspect? (or 'done' to finish): \").strip()\n",
    "    \n",
    "    if follow_up.lower() in ['done', 'no', 'n', '']:\n",
    "        satisfied = True\n",
    "        follow_up = \"\"\n",
    "    else:\n",
    "        satisfied = False\n",
    "    \n",
    "    return {\n",
    "        \"user_satisfied\": satisfied,\n",
    "        \"follow_up_requested\": follow_up,\n",
    "        \"current_query\": follow_up,  # New search query\n",
    "        \"research_complete\": False if not satisfied else True,  # Reset if more research needed\n",
    "        \"iteration_count\": 1 if not satisfied else state[\"iteration_count\"]  # Reset counter for follow-up\n",
    "    }\n",
    "\n",
    "# Conditional routing functions\n",
    "def should_continue_research(state: ResearchState):\n",
    "    \"\"\"Decide whether to continue research loop\"\"\"\n",
    "    if state[\"research_complete\"]:\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"search\"\n",
    "\n",
    "def should_ask_user(state: ResearchState):\n",
    "    \"\"\"Decide whether to ask user for satisfaction\"\"\"\n",
    "    return \"ask_user\" if state[\"research_complete\"] else \"continue\"\n",
    "\n",
    "def should_finish(state: ResearchState):\n",
    "    \"\"\"Decide whether to finish or continue with follow-up\"\"\"\n",
    "    if state[\"user_satisfied\"]:\n",
    "        return \"finish\"\n",
    "    else:\n",
    "        return \"search\"  # Start new research cycle\n",
    "\n",
    "def visualize_data():\n",
    "    pass\n",
    "\n",
    "\n",
    "# Build the research workflow graph\n",
    "research_workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add nodes\n",
    "research_workflow.add_node(\"search\", search_web)\n",
    "research_workflow.add_node(\"analyze\", analyze_completeness)\n",
    "research_workflow.add_node(\"generate_answer\", generate_final_answer)\n",
    "research_workflow.add_node(\"ask_user\", ask_user_satisfaction)\n",
    "\n",
    "\n",
    "#Plannning     \n",
    "research_workflow.add_edge(START, \"search\")\n",
    "research_workflow.add_edge(\"search\", \"analyze\")\n",
    "\n",
    "# ‚úÖ LOOP: analyze can route back to search!\n",
    "research_workflow.add_conditional_edges(\n",
    "    \"analyze\",\n",
    "    should_continue_research,\n",
    "    {\n",
    "        \"search\": \"search\",  # ‚úÖ LOOP BACK!\n",
    "        \"generate_answer\": \"generate_answer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "research_workflow.add_edge(\"generate_answer\", \"ask_user\")\n",
    "\n",
    "# ‚úÖ LOOP: user can request more research!\n",
    "research_workflow.add_conditional_edges(\n",
    "    \"ask_user\",\n",
    "    should_finish,\n",
    "    {\n",
    "        \"search\": \"search\",  # ‚úÖ LOOP BACK for follow-up!\n",
    "        \"finish\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "research_app = research_workflow.compile()\n",
    "\n",
    "print(\"üï∏Ô∏è LangGraph iterative research workflow compiled with native loop support!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Iteration 1: Searching for 'how much electricity will another ai data center from big tech company use?'\n",
      "üìä Analysis complete: Research continuing\n",
      "üéØ Next query: \"sustainable practices in AI data centers and their impact on carbon emissions\"\n",
      "üîç Iteration 2: Searching for '\"sustainable practices in AI data centers and their impact on carbon emissions\"'\n",
      "üìä Analysis complete: Research continuing\n",
      "üéØ Next query: \"case studies on sustainable AI practices in data centers\"\n",
      "üîç Iteration 3: Searching for '\"case studies on sustainable AI practices in data centers\"'\n",
      "üìä Analysis complete: Research finished\n",
      "‚úÖ Final answer generated\n",
      "\n",
      "============================================================\n",
      "üéØ RESEARCH COMPLETE\n",
      "============================================================\n",
      "### Environmental Impacts of AI and Machine Learning\n",
      "\n",
      "The rise of artificial intelligence (AI) and machine learning (ML) technologies has brought significant advancements across various sectors, but it also poses notable environmental challenges. This synthesis examines the environmental impacts of AI, particularly focusing on its carbon footprint, energy consumption, and potential sustainable practices.\n",
      "\n",
      "#### 1. **Carbon Footprint of AI**\n",
      "\n",
      "AI and ML models, especially large and complex ones, have a substantial carbon footprint primarily due to the energy-intensive processes involved in their training and operation. Research indicates that the carbon emissions from training models like GPT-3 can reach approximately **552 metric tons of CO2 equivalent (CO2e)** (Nature, 2024). The overall carbon footprint of AI is estimated to account for **2.1% to 3.9%** of the total carbon emissions from the information and communications technology (ICT) sector (ACM, 2021).\n",
      "\n",
      "The operational emissions dominate the lifecycle emissions of AI hardware, accounting for **70% to 90%** of total emissions, while manufacturing emissions contribute less than **25%** (arXiv, 2023). This highlights the importance of energy consumption during the operational phase of AI systems.\n",
      "\n",
      "#### 2. **Energy Consumption of AI Data Centers**\n",
      "\n",
      "AI data centers are significant consumers of electricity. Projections suggest that by **2030**, these centers will require an additional **18 gigawatts** of power capacity, which is comparable to the total power demand of New York City (CBS News, 2023). By **2028**, it is estimated that AI-specific operations could consume between **165 and 326 terawatt-hours** of electricity annually, potentially powering **22% of U.S. households** (Technology Review, 2025).\n",
      "\n",
      "The energy consumption in data centers is primarily driven by computing power and cooling systems, which together account for about **80%** of total power usage (Deloitte, 2025). As AI workloads become more prevalent, the demand for energy-efficient solutions becomes increasingly critical.\n",
      "\n",
      "#### 3. **Sustainable Practices in AI Data Centers**\n",
      "\n",
      "To mitigate the environmental impact of AI, several sustainable practices are being adopted in data centers:\n",
      "\n",
      "- **Renewable Energy Sources**: Transitioning to renewable energy sources such as solar, wind, and hydroelectric power can significantly reduce the carbon emissions associated with AI operations. Companies like Google and Microsoft have committed to running their data centers entirely on renewable energy (Verne Global, 2023).\n",
      "\n",
      "- **AI-Driven Optimization**: AI can be employed to enhance the efficiency of data center operations. For instance, algorithms can dynamically allocate resources and optimize cooling systems, leading to reduced energy consumption. Google DeepMind has implemented AI-powered cooling systems that adjust settings based on real-time data, resulting in significant energy savings (Medium, 2023).\n",
      "\n",
      "- **Lifecycle Emissions Management**: Understanding and managing the lifecycle emissions of AI hardware can help reduce the overall environmental impact. Strategies include optimizing hardware utilization and improving the efficiency of manufacturing processes (arXiv, 2023).\n",
      "\n",
      "#### 4. **Case Studies and Industry Initiatives**\n",
      "\n",
      "Several tech giants are actively pursuing sustainable AI practices:\n",
      "\n",
      "- **Microsoft**: The company uses AI algorithms to improve energy efficiency in its data centers by scheduling workloads dynamically, allowing servers to enter low-power states during periods of decreased demand (Digital Realty, 2023).\n",
      "\n",
      "- **Google DeepMind**: This initiative has successfully reduced energy consumption in data centers through intelligent cooling management, showcasing how AI can drive sustainability in operations (Medium, 2023).\n",
      "\n",
      "These examples illustrate the potential for AI not only to contribute to environmental challenges but also to offer solutions that enhance sustainability.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "The environmental impacts of AI and machine learning are multifaceted, encompassing significant carbon emissions and energy consumption. However, with the adoption of sustainable practices and innovative technologies, there is a pathway to mitigate these impacts. As AI continues to evolve, it is crucial for organizations to prioritize sustainability to ensure that the benefits of AI outweigh its environmental costs. The future of AI must be aligned with efforts to reduce its carbon footprint and promote energy efficiency, paving the way for a more sustainable digital landscape.\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Run a shorter demo version\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning automated research demo...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m final_demo = \u001b[43mresearch_app\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdemo_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/lc-academy-env/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/lc-academy-env/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/lc-academy-env/lib/python3.11/site-packages/langgraph/pregel/runner.py:161\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    159\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/lc-academy-env/lib/python3.11/site-packages/langgraph/pregel/retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/lc-academy-env/lib/python3.11/site-packages/langgraph/utils/runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/lc-academy-env/lib/python3.11/site-packages/langgraph/utils/runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36mask_user_satisfaction\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[32m    147\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m follow_up = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43müë§ Would you like more detail on any specific aspect? (or \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdone\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m to finish): \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_up.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mdone\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mno\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mn\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    152\u001b[39m     satisfied = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/lc-academy-env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.0/envs/lc-academy-env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def interactive_research():\n",
    "    \"\"\"Interactive research session with natural loops\"\"\"\n",
    "    print(\"ü§ñ Welcome to the LangGraph Iterative Research Assistant!\")\n",
    "    print(\"I'll conduct thorough research and ask follow-up questions as needed.\")\n",
    "    print(\"The system will automatically determine when more research is required.\\n\")\n",
    "    \n",
    "    # Get user's research question\n",
    "    user_question = input(\"üë§ What would you like me to research in depth? \")\n",
    "    \n",
    "    if not user_question.strip():\n",
    "        print(\"Please provide a research question!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize research state\n",
    "    initial_state = {\n",
    "        \"original_question\": user_question,\n",
    "        \"current_query\": user_question,\n",
    "        \"search_results\": [],\n",
    "        \"analysis_history\": [],\n",
    "        \"iteration_count\": 1,\n",
    "        \"research_complete\": False,\n",
    "        \"information_gaps\": [],\n",
    "        \"final_answer\": \"\",\n",
    "        \"user_satisfied\": False,\n",
    "        \"follow_up_requested\": \"\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüî¨ Starting iterative research on: '{user_question}'\")\n",
    "    print(\"üîÑ The system will automatically loop until comprehensive answer is found...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Run the research workflow - loops automatically!\n",
    "        final_state = research_app.invoke(initial_state)\n",
    "        \n",
    "        print(\"\\nüéâ Research session completed!\")\n",
    "        print(f\"üìä Total iterations: {final_state['iteration_count'] - 1}\")\n",
    "        print(f\"üîç Search queries used: {len(final_state['search_results'])}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüëã Research session interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during research: {e}\")\n",
    "\n",
    "# Uncomment to start interactive research:\n",
    "interactive_research()\n",
    "\n",
    "print(\"üí° Tip: Uncomment the line above to try interactive iterative research!\")\n",
    "print(\"\\nSample research topics to try:\")\n",
    "print(\"1. 'What are the latest breakthroughs in quantum computing?'\")\n",
    "print(\"2. 'How is artificial intelligence being used in healthcare?'\")\n",
    "print(\"3. 'What are the environmental effects of electric vehicles?'\")\n",
    "print(\"4. 'How do social media algorithms affect mental health?'\")\n",
    "\n",
    "# Demo with a fixed example\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üî¨ DEMO: Automatic Iterative Research\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "demo_state = {\n",
    "    \"original_question\": \"What are the environmental impacts of AI and machine learning?\",\n",
    "    \"current_query\": \"environmental impacts AI machine learning carbon footprint\",\n",
    "    \"search_results\": [],\n",
    "    \"analysis_history\": [],\n",
    "    \"iteration_count\": 1,\n",
    "    \"research_complete\": False,\n",
    "    \"information_gaps\": [],\n",
    "    \"final_answer\": \"\",\n",
    "    \"user_satisfied\": True,  # Skip user interaction for demo\n",
    "    \"follow_up_requested\": \"\"\n",
    "}\n",
    "\n",
    "# Run a shorter demo version\n",
    "print(\"Running automated research demo...\")\n",
    "final_demo = research_app.invoke(demo_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Benefits of LangGraph for Loops:\n",
    "\n",
    "1. **Native Loop Support** - Cycles are first-class citizens in the graph\n",
    "2. **Conditional Routing** - Smart decisions about when to loop\n",
    "3. **State Persistence** - Information accumulates across iterations\n",
    "4. **Flexible Control Flow** - Can loop back from any node\n",
    "5. **Error Recovery** - Failed iterations don't break the entire flow\n",
    "6. **User Interaction** - Natural follow-up questions and refinement\n",
    "7. **Termination Conditions** - Multiple ways to exit loops gracefully\n",
    "8. **Visual Workflow** - Easy to understand and debug loop logic\n",
    "\n",
    "**üß† The key insight**: LangGraph treats **loops as natural workflow patterns**, not exceptional cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Side-by-Side Comparison\n",
    "\n",
    "## üîó LangChain Approach:\n",
    "```python\n",
    "# ‚ùå Manual while loop (external to framework)\n",
    "while iteration < max_iterations:\n",
    "    search_results = search_tool.invoke(query)\n",
    "    analysis = analysis_chain.run(results=search_results)\n",
    "    \n",
    "    # ‚ùå Brittle text parsing\n",
    "    if \"SUFFICIENT: Yes\" in analysis:\n",
    "        break  # Manual loop control\n",
    "    \n",
    "    # ‚ùå Extract next query manually\n",
    "    query = parse_next_query(analysis)\n",
    "\n",
    "# Problems:\n",
    "# - External loop control\n",
    "# - Brittle condition checking\n",
    "# - No state management\n",
    "# - Hard to handle errors\n",
    "```\n",
    "\n",
    "## üï∏Ô∏è LangGraph Approach:\n",
    "```python\n",
    "# ‚úÖ Natural loop as part of workflow\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze\",\n",
    "    should_continue_research,\n",
    "    {\n",
    "        \"search\": \"search\",      # ‚úÖ Loop back!\n",
    "        \"generate_answer\": \"generate_answer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"ask_user\",\n",
    "    should_finish,\n",
    "    {\n",
    "        \"search\": \"search\",      # ‚úÖ Follow-up loop!\n",
    "        \"finish\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Benefits:\n",
    "# - Native loop support\n",
    "# - Declarative conditions\n",
    "# - Automatic state management\n",
    "# - Built-in error handling\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ When Do You Need Loops?\n",
    "\n",
    "## üîÑ **Common Loop Patterns in AI:**\n",
    "\n",
    "### 1. **Iterative Refinement**\n",
    "- Research that builds on previous findings\n",
    "- Code generation with debugging cycles\n",
    "- Creative writing with revision loops\n",
    "\n",
    "### 2. **Progressive Elaboration**\n",
    "- Start with high-level overview\n",
    "- Drill down into specific details\n",
    "- User-guided exploration\n",
    "\n",
    "### 3. **Trial and Error**\n",
    "- Multiple search strategies\n",
    "- Fallback approaches when primary fails\n",
    "- A/B testing different prompts\n",
    "\n",
    "### 4. **User Interaction Cycles**\n",
    "- Chatbot conversations\n",
    "- Interactive tutorials\n",
    "- Multi-turn decision making\n",
    "\n",
    "### 5. **Conditional Processing**\n",
    "- \"Keep searching until satisfied\"\n",
    "- \"Try different approaches until success\"\n",
    "- \"Refine until quality threshold met\"\n",
    "\n",
    "## üö´ **When You DON'T Need Loops:**\n",
    "- Simple question ‚Üí answer workflows\n",
    "- One-time document processing\n",
    "- Static data transformations\n",
    "- Batch processing independent items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Key Takeaways\n",
    "\n",
    "## üîç **The Fundamental Difference**:\n",
    "- **LangChain**: Linear chains (DAGs) - **no cycles allowed**\n",
    "- **LangGraph**: State machines - **loops are natural**\n",
    "\n",
    "## üîÑ **Loop Implementation**:\n",
    "- **LangChain**: External while loops (hacky, brittle)\n",
    "- **LangGraph**: Conditional edges (elegant, robust)\n",
    "\n",
    "## üß† **Mental Models**:\n",
    "- **LangChain**: Assembly line - each step happens once\n",
    "- **LangGraph**: Workflow engine - steps can repeat as needed\n",
    "\n",
    "## üéØ **The Decision Questions**:\n",
    "1. **Do you need to repeat steps based on conditions?** ‚Üí LangGraph\n",
    "2. **Will users want to refine or iterate?** ‚Üí LangGraph\n",
    "3. **Is it a simple linear process?** ‚Üí LangChain\n",
    "4. **Do you need \"keep trying until success\"?** ‚Üí LangGraph\n",
    "\n",
    "## üí° **Pro Tips**:\n",
    "- **LangChain** is perfect for **linear, one-shot workflows**\n",
    "- **LangGraph** shines when you need **iterative, adaptive processes**\n",
    "- **Loops** are essential for **research, refinement, and interaction**\n",
    "- **State management** makes complex workflows much easier\n",
    "\n",
    "## üöÄ **Real-World Examples**:\n",
    "- **Research Assistant**: Keep searching until comprehensive\n",
    "- **Code Debugger**: Try fixes until tests pass\n",
    "- **Creative Writer**: Refine until user satisfied\n",
    "- **Data Analyst**: Explore until insights found\n",
    "- **Tutor**: Explain until student understands\n",
    "\n",
    "## üîÆ **The Future is Iterative**:\n",
    "AI workflows are becoming more **sophisticated** and **interactive**. The ability to **loop, adapt, and refine** is crucial for building truly intelligent systems.\n",
    "\n",
    "**Remember**: If your AI needs to **\"keep trying until...\"** - you need LangGraph! üîÑ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
